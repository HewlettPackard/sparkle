# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
# spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.driver.memory              {{driver_memory}}
spark.executor.memory            {{executor_memory}}

##to control Netty dispatcher threads for workers/executors.
spark.rpc.netty.dispatcher.numThreads  {{worker_cores}}

##to control HTTP Server and Jetty Web Server dispatcher threads
spark.jetty.httpserver.numThreads   {{worker_cores}}

# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
# spark.cores.max         19


spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.kryo.registrator           org.apache.spark.graphx.GraphKryoRegistrator
spark.eventLog.enabled           true
spark.eventLog.dir               {{work_dir}}/store-w{{worker}}/eventLog
spark.local.dir                         {{work_dir}}/store-w{{worker}}/local
spark.log.Conf                          true
spark.eventLog.compress                 false
spark.ui.retainedStages                 10000
#spark.default.parallelism	 12

##to get around the kryo serializer buffer limit problem
spark.kryoserializer.buffer.max.mb 1024
#spark.executor.extraClassPath   /usr/local/hadoop/extras/jblas-master/target/jblas-1.2.4-SNAPSHOT.jar
# spark.executor.extraLibraryPath /usr/lib64

# spark.sql.dialect sql
# hiveql parser is more robust and allows access to hive udfs.
##spark.sql.dialect hiveql

# this is needed when creating files/tables and switching between spark-scala-shell,
# spark-sql-shell, and jdbc such that they can all read/write the parquet files correctly
# to share among all shell flavors.
spark.sql.parquet.binaryAsString true
##spark.sql.hive.version=0.12.0

#/usr/local/hadoop/hadoop-2.5.1/lib/native
#spark.executor.extraClassPath   /usr/local/hadoop/extras/sparkjars/netlib-native_system-linux-x86_64-1.1-natives.jar
#spark.executor.extraClassPath   /usr/local/hadoop/extras/sparkjars/
##spark.executor.extraClassPath   /usr/local/hadoop/spark-1.1.0/assembly/target/scala-2.10/spark-assembly-1.1.0-hadoop2.4.0.jar
##spark.executor.extraClassPath   /usr/local/hadoop/spark-1.1.0/assembly/target/scala-2.10
#spark.executor.extraClassPath   /usr/local/hadoop/spark-1.1.0/assembly/target/scala-2.10/spark-assembly-1.1.0-hadoop2.4.0.jar
# spark.executor.extraClassPath   /usr/local/hadoop/extras/sparkjars/spark-assembly-1.1.0-hadoop2.4.0.jar
#spark.executor.extraClassPath   /usr/local/hadoop/extras/sparkjars/netlib-native_system-linux-x86_64-1.1-natives.jar

spark.driver.maxResultSize   0


