# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
# spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.driver.memory              {{driver_memory}}
spark.executor.memory            {{executor_memory}}

##to control Netty dispatcher threads for workers/executors.
spark.rpc.netty.dispatcher.numThreads  10

##to control HTTP Server and Jetty Web Server dispatcher threads
spark.jetty.httpserver.numThreads 10

# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
# spark.cores.max         19


spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.kryo.referenceTracking     false
spark.kryo.registrator           org.apache.spark.graphx.GraphKryoRegistrator
spark.eventLog.enabled           true
##NOTE: This will have to be changed to prefix with individual worker's local directory
spark.eventLog.dir               {{work_dir}}/store-m0/eventLog
##NOTE: This will have to be changed to prefix with individual worker's local directory
spark.local.dir                         {{work_dir}}/store-m0/local
spark.log.Conf                          true
spark.eventLog.compress                 false
spark.ui.retainedStages                 10000
#spark.default.parallelism	 12

##to get around the kryo serializer buffer limit problem
spark.kryoserializer.buffer.max.mb 1024
#spark.executor.extraClassPath   /usr/local/hadoop/extras/jblas-master/target/jblas-1.2.4-SNAPSHOT.jar
# spark.executor.extraLibraryPath /usr/lib64

# spark.sql.dialect sql
# hiveql parser is more robust and allows access to hive udfs.
##spark.sql.dialect hiveql

# this is needed when creating files/tables and switching between spark-scala-shell,
# spark-sql-shell, and jdbc such that they can all read/write the parquet files correctly
# to share among all shell flavors.
spark.sql.parquet.binaryAsString true
##spark.sql.hive.version=0.12.0


spark.driver.maxResultSize   0
