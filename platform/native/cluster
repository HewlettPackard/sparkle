#!/usr/bin/python

# * (c) Copyright 2016 Hewlett Packard Enterprise Development LP
# *
# * Licensed under the Apache License, Version 2.0 (the "License");
# * you may not use this file except in compliance with the License.
# * You may obtain a copy of the License at
# *
# *     http://www.apache.org/licenses/LICENSE-2.0
# *
# * Unless required by applicable law or agreed to in writing, software
# * distributed under the License is distributed on an "AS IS" BASIS,
# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# * See the License for the specific language governing permissions and
# * limitations under the License.
#

import sys
import os
import re
import fileinput
import argparse
import getpass
import shutil
import socket
import string
import subprocess
import tempfile
import ConfigParser

sys.path.append("../common")
import vnode



def execute_cmd(argv, sudo=False):
    if sudo:
        argv = ['sudo'] + argv
    p = subprocess.Popen(argv, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False)
    out, err = p.communicate()
    return out, err

def execute_shell(sh):
    p = subprocess.Popen(sh, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
    out, err = p.communicate()
    return out, err

def mount_tmpfs(mnt_path, size, memnode_id):
    out, err = execute_cmd(['mount', '-t', 'tmpfs', '-o', 'noatime,size=%s,mpol=bind:%d' % (size, memnode_id), 'tmpfs', mnt_path], True)

def mount_ext4(mnt_path, block_dev):
    out, err = execute_cmd(['mount', '-t', 'ext4', '-o', 'noatime', block_dev, mnt_path], True)
    out, err = execute_cmd(['chmod', 'a+rw', mnt_path], True)

def umount_fs(mnt_path):
    out, err = execute_cmd(['umount', mnt_path], True)

def get_nvmfs_factory(nvmfs_type, pmem_dev=None):
    if nvmfs_type == 'tmpfs':
       return NvmFsFactoryTmpfs()
    elif nvmfs_type == 'ext4':
       return NvmFsFactoryExt4(pmem_dev)

class NvmFsFactoryExt4:
    def __init__(self, pmem_dev=None):
        self.mnt_path = '/mnt/sharedfs'
        self.pmem_dev = pmem_dev
        out, err = execute_cmd(['mkdir', '-p', self.mnt_path], True)
        if not os.path.ismount(self.mnt_path):
            mount_ext4(self.mnt_path, pmem_dev)

    def create(self, path, size, memnode_id):
        os.makedirs(os.path.join(self.mnt_path, path[1:]))
        os.symlink(os.path.join(self.mnt_path, path[1:]), path)
        
    def destroy(self, path):
        umount_fs(self.mnt_path)

class NvmFsFactoryTmpfs:
    def create(self, path, size, memnode_id):
        os.makedirs(path)
        mount_tmpfs(path, size, memnode_id)

    def destroy(self, path):
        umount_fs(path)


def sed(file_path, pattern, repl):
    f = open(file_path)
    fd, new_file_path = tempfile.mkstemp()
    new_file = open(new_file_path, 'w')
    for l in f.readlines():
        rl = re.sub(pattern, repl, l)
        new_file.write(rl)
    new_file.close()
    f.close()	
    shutil.move(new_file_path, file_path)

class SparkImage:
    def __init__(self, spark_src_path, sandbox, vnode_id, num_vnodes, num_worker_cores):
        self.spark_src_path = spark_src_path
        self.sandbox_path = sandbox.path()
        self.vnode_id = vnode_id
        self.num_vnodes = num_vnodes
        self.spark_path = os.path.join(self.sandbox_path, "spark")
        self.local_dir_path = os.path.join(self.sandbox_path, "tmp/spark")
        self.num_worker_cores = num_worker_cores
        self.ht_enabled = True

    def install_image(self, src_path, dest_path):
        if not os.path.exists(dest_path):
            os.mkdir(dest_path)
        os.mkdir(os.path.join(dest_path, "pids"))
        os.mkdir(os.path.join(dest_path, "logs"))
        os.mkdir(os.path.join(dest_path, "work"))
        shutil.copytree("template/conf", os.path.join(dest_path, "conf"))  
        shutil.copytree("template/bin", os.path.join(dest_path, "bin"))  
        shutil.copytree("template/sbin", os.path.join(dest_path, "sbin"))  
        os.symlink(os.path.join(src_path, "assembly"), os.path.join(dest_path, "assembly"))
        os.symlink(os.path.join(src_path, "lib_managed"), os.path.join(dest_path, "lib_managed"))
        os.symlink(os.path.join(src_path, "tools"), os.path.join(dest_path, "tools"))
        os.mkdir(self.local_dir_path)

    def install_master_image(self, src_path, dest_path):
        self.install_image(src_path, dest_path)
            
    def install_worker_image(self, src_path, dest_path):
        self.install_image(src_path, dest_path)
        
    def config(self, sandbox_path, spark_path):
        spark_defaults_path = os.path.join(spark_path, "conf/spark-defaults.conf")
        eventlog_dir_path = os.path.join(sandbox_path, "tmp/spark-events")
        sed(spark_defaults_path, "__EVENTLOG_DIR__", eventlog_dir_path)
        sed(spark_defaults_path, "__LOCAL_DIR__", self.local_dir_path)

        spark_env_path = os.path.join(spark_path, "conf/spark-env.sh")
        worker_dir_path = os.path.join(spark_path, "work")
        sed(spark_env_path, "__SPARK_MASTER_HOSTNAME__", socket.gethostname())
        num_worker_cores = vnode.cpunum(self.vnode_id, self.num_vnodes, self.ht_enabled, 1024)
        if self.num_worker_cores and int(self.num_worker_cores) < num_worker_cores:
            num_worker_cores = int(self.num_worker_cores)
        sed(spark_env_path, "__SPARK_WORKER_CORES__", str(num_worker_cores))
        sed(spark_env_path, "__SPARK_WORKER_DIR__", worker_dir_path)
        sed(spark_env_path, "__SPARK_LOCAL_DIR__", self.local_dir_path)
        sed(spark_env_path, "__SPARK_MEMNODE_AFFINITY__", str(vnode.vnode_to_memnode(self.vnode_id)))
        sed(spark_env_path, "__SPARK_CORE_AFFINITY__", vnode.cpuset_str(self.vnode_id, self.num_vnodes, self.ht_enabled, 1024))

    def config_master(self, sandbox_path, spark_path):
        self.config(sandbox_path, spark_path)
        spark_env_path = os.path.join(spark_path, "conf/spark-env.sh")
        sed(spark_env_path, "SPARK_WORKER_WEBUI_PORT=", "#SPARK_WORKER_WEBUI_PORT=")

    def config_worker(self, sandbox_path, spark_path):
        self.config(sandbox_path, spark_path)
        spark_env_path = os.path.join(spark_path, "conf/spark-env.sh")
        sed(spark_env_path, "__SPARK_WORKER_WEBUI_PORT__", str(8081 + self.vnode_id))

    def deploy_master(self):
        self.install_master_image(self.spark_src_path, self.spark_path)
        self.config_master(self.sandbox_path, self.spark_path)    

    def deploy_worker(self):
        self.install_worker_image(self.spark_src_path, self.spark_path)
        self.config_worker(self.sandbox_path, self.spark_path)    


def deploy_spark_worker_image(src_path, dest_path):
    shutil.copytree("template", dest_path)  


class Sandbox:
    def __init__(self, root_dir, sandbox_name, vnode_id, nvmfs_factory, nvmfs_size=None):
        self.root_dir = root_dir
        self.sandbox_name = sandbox_name
        self.sandbox_path = os.path.join(root_dir, sandbox_name)
        self.vnode_id = vnode_id
        self.memnode_id = vnode.vnode_to_memnode(vnode_id)
        self.nvmfs_factory = nvmfs_factory
        self.nvmfs_size = nvmfs_size

    def create(self):
        print("Creating sandbox: %s" % (self.sandbox_name))
        tmp_path = os.path.join(self.sandbox_path, "tmp")
        os.makedirs(self.sandbox_path)
        self.nvmfs_factory.create(tmp_path, self.nvmfs_size, self.memnode_id) 

    def destroy(self):
        if os.path.exists(self.sandbox_path):
            print("Destroying sandbox: %s" % (self.sandbox_name))
            tmp_path = os.path.join(self.sandbox_path, "tmp")
            self.nvmfs_factory.destroy(tmp_path) 
            shutil.rmtree(self.sandbox_path)
    
    def name(self):
        return self.sandbox_name

    def path(self):
        return self.sandbox_path

class CreateCluster:
    def __init__(self, args):
        self.args = args

    def create_master_node(self, root_dir, nvmfs_factory, nvmfs_size):
        vnode_id = 0
        sandbox = Sandbox(root_dir, "vnode-master", vnode_id, nvmfs_factory, nvmfs_size)
        sandbox.create()
        print("Creating sandbox: %s, nvmfs size: %s" % (sandbox.name(), nvmfs_size))
        SparkImage(self.args.spark_bin_dir, sandbox, vnode_id, int(self.args.num_vnodes), self.args.num_worker_cores).deploy_master()

    def create_worker_node(self, root_dir, vnode_id, nvmfs_factory, nvmfs_size):
        sandbox = Sandbox(root_dir, "vnode-worker%d" % vnode_id, vnode_id, nvmfs_factory, nvmfs_size)
        sandbox.create()
        print("Creating sandbox: %s, nvmfs size: %s" % (sandbox.name(), nvmfs_size))
        SparkImage(self.args.spark_bin_dir, sandbox, vnode_id, int(self.args.num_vnodes), self.args.num_worker_cores).deploy_worker()
        return sandbox
       
    def create(self):
        nvmfs_factory = get_nvmfs_factory(self.args.nvmfs_type, self.args.pmem_dev)
        self.create_master_node(self.args.root_dir, nvmfs_factory, self.args.master_nvmfs_size)
        sandboxes = []
        for vid in range(int(self.args.num_vnodes)):
            sandbox = self.create_worker_node(self.args.root_dir, vid, nvmfs_factory, self.args.worker_nvmfs_size)
            sandboxes.append(sandbox)
        # save cluster configuration 
        config = ConfigParser.RawConfigParser()
        config.add_section('main')
        config.set('main', 'nvmfs-type', self.args.nvmfs_type)
        with open(os.path.join(self.args.root_dir, 'cluster.cfg'), 'wb') as configfile:
            config.write(configfile)

    @staticmethod
    def add_parser(subparsers):
        parser = subparsers.add_parser('create', help = 'create help')
        parser.add_argument('--root', dest='root_dir', default = '/var/tmp',
                            help='directory where cluster node sandboxes are deployed')
        parser.add_argument('--spark-bin', dest='spark_bin_dir', default = '/opt/spark',
                            help='directory containing spark binaries')
        parser.add_argument('--nodes', dest='num_vnodes', default = vnode.numsockets(),
                            help='number of virtual nodes')
        parser.add_argument('--worker-cores', dest='num_worker_cores', required=False,
                            help='number of worker cores')
        parser.add_argument('--master-nvmfs-size', dest='master_nvmfs_size', default = '32g',
                            help='nvmfs size per master node')
        parser.add_argument('--worker-nvmfs-size', dest='worker_nvmfs_size', default = '32g',
                            help='nvmfs size per worker node')
        parser.add_argument('--nvmfs-type', dest='nvmfs_type', default = 'tmpfs',
                            help='nvmfs type: tmpfs, ext4')
        parser.add_argument('--pmem-dev', dest='pmem_dev', default = '/dev/pmem0',
                            help='pmem device path')




class DestroyCluster:
    def __init__(self, args):
        self.args = args

    def destroy_sandbox(self, root_dir, sandbox_name):
        sandbox_path = os.path.join(root_dir, sandbox_name)
        if os.path.exists(sandbox_path):
            tmp_path = os.path.join(sandbox_path, "tmp")
            umount_fs(tmp_path) 
            shutil.rmtree(sandbox_path)

    def destroy_master_sandbox(self, root_dir, nvmfs_factory):
        sandbox = Sandbox(root_dir, "vnode-master", 0, nvmfs_factory)
        sandbox.destroy()

    def destroy_worker_sandbox(self, root_dir, vnode_id, nvmfs_factory):
        sandbox = Sandbox(root_dir, "vnode-worker%d" % (vnode_id), vnode_id, nvmfs_factory)
        sandbox.destroy()

    def destroy(self):
        # load cluster configuration 
        config = ConfigParser.RawConfigParser()
        config.read(os.path.join(self.args.root_dir, 'cluster.cfg'))
        if self.args.nvmfs_type: 
            nvmfs_type = self.args.nvmfs_type
        else:
            nvmfs_type = config.get('main', 'nvmfs-type')
        nvmfs_factory = get_nvmfs_factory(nvmfs_type)
        self.destroy_master_sandbox(self.args.root_dir, nvmfs_factory)
        for vid in range(int(self.args.num_vnodes)):
            self.destroy_worker_sandbox(self.args.root_dir, vid, nvmfs_factory)

    @staticmethod
    def add_parser(subparsers):
        parser = subparsers.add_parser('destroy', help = 'destroy help')
        parser.add_argument('--root', dest='root_dir', default = '/var/tmp',
                            help='directory where cluster sandboxs are deployed')
        parser.add_argument('--nodes', dest='num_vnodes', default = vnode.numsockets(),
                            help='number of virtual nodes')
        parser.add_argument('--nvmfs-type', dest='nvmfs_type', default = None,
                            help='force nvmfs type: tmpfs, ext4')




def main(argv):
    parser = argparse.ArgumentParser(description='Manage Spark cluster.')
   
    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')
    CreateCluster.add_parser(subparsers)
    DestroyCluster.add_parser(subparsers)

    args = parser.parse_args(argv)

    if args.subparser_name == 'create':
        CreateCluster(args).create()
    if args.subparser_name == 'destroy':
        DestroyCluster(args).destroy()

    return
 

if __name__ == "__main__":
    main(sys.argv[1:])
